{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad07b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup Logging ---\n",
    "# Mengatur sistem logging untuk mencatat informasi, peringatan, dan kesalahan.\n",
    "# Log akan ditampilkan di konsol dan juga disimpan ke file log.\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b682933",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- Konfigurasi Path ---\n",
    "# Mendefinisikan semua direktori input dan output. Menggunakan pathlib.Path \n",
    "# untuk penanganan path yang lebih modern dan platform-agnostik.\n",
    "INPUT_DIR = Path(\"data/pdf\")\n",
    "RAW_TXT_DIR = Path(\"data/raw\")\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "OUTPUT_CASES_JSON = PROCESSED_DIR / \"cases.json\" # Menyimpan semua metadata kasus\n",
    "OUTPUT_TEXT_MAP_JSON = PROCESSED_DIR / \"case_raw_texts.json\" # Menyimpan teks mentah (opsional, bisa dihilangkan jika tidak diperlukan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292cc5ad",
   "metadata": {},
   "source": [
    "--- Fungsi Utilitas Direktori ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903dcc4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ensure_directories():\n",
    "    \"\"\"\n",
    "    Memastikan semua direktori yang diperlukan untuk input dan output sudah ada.\n",
    "    Jika sebuah direktori belum ada, fungsi ini akan membuatnya.\n",
    "    \"\"\"\n",
    "    for directory in [INPUT_DIR, RAW_TXT_DIR, PROCESSED_DIR]:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Directory ensured: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c886c6",
   "metadata": {},
   "source": [
    "--- Fungsi Generasi ID ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cfe283",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_case_id_from_filename(filename_stem: str) -> str:\n",
    "    \"\"\"\n",
    "    Menghasilkan case_id yang bersih dari nama file dasar (tanpa ekstensi).\n",
    "    Ini tidak lagi mencakup timestamp karena case_id yang sudah ada harus unik.\n",
    "    Timestamp akan ditambahkan saat kasus diproses.\n",
    "    \n",
    "    Args:\n",
    "        filename_stem (str): Nama file tanpa ekstensi (misal: \"putusan_123_abc\").\n",
    "        \n",
    "    Returns:\n",
    "        str: Case ID yang dibersihkan.\n",
    "    \"\"\"\n",
    "    # Bersihkan nama file: hapus karakter non-alfanumerik atau non-underscore/dash, \n",
    "    # ganti spasi dengan underscore, hindari underscore berturut-turut.\n",
    "    clean_name = re.sub(r'[^\\w\\-.]', '_', filename_stem.lower())\n",
    "    clean_name = re.sub(r'_+', '_', clean_name).strip('_')\n",
    "    return clean_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6f4cb",
   "metadata": {},
   "source": [
    "--- Fungsi Ekstraksi Teks PDF ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25047809",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_text_blocks_improved(page: fitz.Page) -> str:\n",
    "    \"\"\"\n",
    "    Mengekstrak teks dari halaman PDF menggunakan metode 'blocks' PyMuPDF,\n",
    "    dengan penyortiran yang lebih baik dan filter untuk block yang valid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        if not blocks:\n",
    "            return \"\"\n",
    "        \n",
    "        # Urutkan blok berdasarkan posisi (dari atas ke bawah, dari kiri ke kanan)\n",
    "        # Menggunakan pembulatan untuk menangani sedikit perbedaan koordinat.\n",
    "        blocks.sort(key=lambda b: (round(b[1]), round(b[0])))\n",
    "        \n",
    "        text_parts = []\n",
    "        for block in blocks:\n",
    "            # Format block: [x0, y0, x1, y1, text, block_no, block_type]\n",
    "            if len(block) >= 5 and block[4] and isinstance(block[4], str):\n",
    "                text = block[4].strip()\n",
    "                if text: # Pastikan teks tidak kosong setelah strip\n",
    "                    text_parts.append(text)\n",
    "        \n",
    "        return \"\\n\".join(text_parts)\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Block extraction failed on page (improved): {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ccf1ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_text_dict_improved(page: fitz.Page) -> str:\n",
    "    \"\"\"\n",
    "    Mengekstrak teks dari halaman PDF menggunakan metode 'dict' PyMuPDF,\n",
    "    dengan iterasi yang lebih hati-hati melalui struktur kamus.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text_dict = page.get_text(\"dict\")\n",
    "        if not text_dict or \"blocks\" not in text_dict:\n",
    "            return \"\"\n",
    "        \n",
    "        page_text_parts = []\n",
    "        for block in text_dict.get(\"blocks\", []):\n",
    "            if \"lines\" not in block:\n",
    "                continue\n",
    "            \n",
    "            block_lines = []\n",
    "            for line in block[\"lines\"]:\n",
    "                if \"spans\" not in line:\n",
    "                    continue\n",
    "                \n",
    "                line_text = \"\".join([span.get(\"text\", \"\") for span in line[\"spans\"]])\n",
    "                if line_text.strip():\n",
    "                    block_lines.append(line_text.strip())\n",
    "            \n",
    "            if block_lines:\n",
    "                page_text_parts.append(\" \".join(block_lines)) # Gabungkan baris dalam blok dengan spasi\n",
    "        \n",
    "        return \"\\n\".join(page_text_parts) # Gabungkan blok dengan newline\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Dict extraction failed on page (improved): {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d1d1c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path: Path) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Mengekstrak teks dari file PDF menggunakan berbagai metode PyMuPDF \n",
    "    dan memilih hasil terbaik.\n",
    "    \n",
    "    Args:\n",
    "        file_path (Path): Path ke file PDF.\n",
    "        \n",
    "    Returns:\n",
    "        Optional[str]: Seluruh teks yang diekstrak dari PDF, atau None jika gagal.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        full_text_parts = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            try:\n",
    "                page = doc[page_num]\n",
    "                page_text_candidates = {}\n",
    "                \n",
    "                # Coba berbagai metode ekstraksi dan simpan hasilnya\n",
    "                methods = {\n",
    "                    \"standard_sorted\": lambda p: p.get_text(\"text\", sort=True),\n",
    "                    \"standard_unsorted\": lambda p: p.get_text(\"text\"),\n",
    "                    \"blocks_custom\": extract_text_blocks_improved,\n",
    "                    \"dict_custom\": extract_text_dict_improved\n",
    "                }\n",
    "                \n",
    "                for method_name, method_func in methods.items():\n",
    "                    try:\n",
    "                        result = method_func(page)\n",
    "                        if result and result.strip():\n",
    "                            page_text_candidates[method_name] = result\n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Method '{method_name}' failed on page {page_num + 1} of {file_path.name}: {e}\")\n",
    "                \n",
    "                best_page_text = \"\"\n",
    "                # Pilih teks terbaik: yang paling panjang (setelah dibersihkan)\n",
    "                for method_name, text in page_text_candidates.items():\n",
    "                    cleaned_current_text = clean_extracted_text(text)\n",
    "                    if len(cleaned_current_text) > len(best_page_text):\n",
    "                        best_page_text = cleaned_current_text\n",
    "                        logger.debug(f\"Page {page_num + 1}: Using {method_name} method (length {len(best_page_text)})\")\n",
    "                \n",
    "                if best_page_text:\n",
    "                    full_text_parts.append(f\"\\n=== HALAMAN {page_num + 1} ===\\n\")\n",
    "                    full_text_parts.append(best_page_text)\n",
    "                else:\n",
    "                    logger.warning(f\"No meaningful text extracted from page {page_num + 1} of {file_path.name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing page {page_num + 1} of {file_path.name}: {e}\")\n",
    "                # Lanjutkan ke halaman berikutnya meskipun ada error pada satu halaman\n",
    "                continue \n",
    "        \n",
    "        doc.close()\n",
    "        return \"\\n\".join(full_text_parts).strip()\n",
    "        \n",
    "    except fitz.FileDataError as e:\n",
    "        logger.error(f\"PDF file corrupted or invalid: {file_path}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error opening or processing PDF {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933a6fd",
   "metadata": {},
   "source": [
    "--- Fungsi Pembersihan Teks ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2080b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def clean_extracted_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Membersihkan teks yang diekstrak dari PDF.\n",
    "    Fokus pada normalisasi spasi/baris baru dan penghapusan artefak yang sangat jelas.\n",
    "    Tujuan utama adalah mempertahankan sebanyak mungkin teks asli tanpa pemotongan yang tidak disengaja.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Teks mentah yang diekstrak dari PDF.\n",
    "        \n",
    "    Returns:\n",
    "        str: Teks yang sudah bersih dan dinormalisasi dengan filtering minimal.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Normalisasi spasi dan baris baru\n",
    "    text = text.replace('\\x00', ' ') # Hapus karakter null (sering muncul dari PDF)\n",
    "    text = re.sub(r'[\\r\\n]+', '\\n', text) # Normalisasi CRLF ke LF, hapus multiple newlines menjadi single newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text) # Normalisasi spasi dan tab ganda menjadi single space\n",
    "    text = text.strip() # Hapus spasi/newline di awal/akhir setelah normalisasi\n",
    "\n",
    "    if not text: # Jika teks menjadi kosong setelah normalisasi awal\n",
    "        return \"\"\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line: # Lewati baris yang kosong setelah di-strip\n",
    "            continue\n",
    "        \n",
    "        # 2. Filter baris yang tidak diinginkan (artefak yang sangat spesifik dan jelas)\n",
    "        # Filter ini dirancang untuk sangat konservatif, hanya membuang yang pasti artefak.\n",
    "        \n",
    "        # Filter nomor halaman yang sangat jelas:\n",
    "        # Contoh: \"1\", \"- 2 -\", \"-- 3 --\", \"Page 4\"\n",
    "        if re.fullmatch(r'^\\s*[-_]?\\s*\\d+\\s*[-_]?\\s*$', line, re.IGNORECASE) or \\\n",
    "           re.fullmatch(r'^\\s*[Pp][Aa][Gg][Ee]\\s+\\d+\\s*$', line, re.IGNORECASE):\n",
    "            continue\n",
    "        \n",
    "        # Filter garis horizontal atau deretan simbol sederhana yang berulang:\n",
    "        # Contoh: \"------------\", \"*********\", \"========\"\n",
    "        # Hanya filter jika baris *seluruhnya* terdiri dari satu atau dua karakter non-alphanumeric berulang.\n",
    "        # Ini adalah filter yang sangat ketat untuk menghindari pemotongan teks valid.\n",
    "        if len(line) > 5: # Harus cukup panjang agar dianggap garis pemisah\n",
    "            # Periksa jika semua karakter adalah non-alphanumeric dan non-spasi\n",
    "            if all(not char.isalnum() and not char.isspace() for char in line):\n",
    "                # Dan hanya ada 1 atau 2 karakter unik non-alphanumeric (e.g., hanya '-' atau '*').\n",
    "                if len(set(char for char in line if not char.isspace())) <= 2:\n",
    "                    continue\n",
    "\n",
    "        # Filter baris yang sangat pendek dan tidak mengandung huruf/angka (seringkali sisa-sisa simbol atau numbering yang salah)\n",
    "        if len(line) <= 3 and not re.search(r'[a-zA-Z0-9]', line):\n",
    "            continue\n",
    "        \n",
    "        # Filter Roman numerals jika tampak sebagai penomoran list/sub-bagian yang berdiri sendiri.\n",
    "        # Contoh: \"I.\", \"II.\", \"V.\"\n",
    "        if len(line) <= 5 and re.fullmatch(r'^[ivxlc]+\\.$', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Semua filter yang lebih agresif (misalnya berdasarkan panjang atau komposisi kata)\n",
    "        # dihilangkan sepenuhnya di sini. Teks akan dipertahankan sebisa mungkin.\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    # Gabungkan baris yang bersih dengan satu newline. \n",
    "    # Pada tahap ini, kita memaksimalkan retensi teks. Rekonstruksi paragraf yang lebih canggih\n",
    "    # atau identifikasi bagian-bagian dokumen akan dilakukan di `02_case_representation.py`\n",
    "    # karena di sana kita memiliki konteks lebih baik tentang pola-pola dalam dokumen.\n",
    "    return \"\\n\".join(cleaned_lines).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3fc0c",
   "metadata": {},
   "source": [
    "--- Fungsi Penyimpanan File ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c0c17",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_text_file(content: str, file_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Menyimpan konten teks ke file yang ditentukan.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving text file {file_path}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9e92b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_json_file(data: Any, file_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Menyimpan data (umumnya daftar kasus atau kamus) ke file JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"JSON saved: {file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving JSON file {file_path}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeae082",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_existing_cases_data(file_path: Path) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Memuat data kasus yang sudah ada dari file JSON. \n",
    "    Mengembalikan kamus yang dipetakan berdasarkan case_id untuk pencarian cepat.\n",
    "    \"\"\"\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    # Mengubah list kasus menjadi dict {case_id: case_obj}\n",
    "                    return {c.get('case_id', generate_case_id_from_filename(f\"temp_{idx}\")): c \n",
    "                            for idx, c in enumerate(data) if c.get('case_id')}\n",
    "                logger.warning(f\"Existing file {file_path} is not a list. Starting fresh.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading existing data from {file_path}: {e}. Starting fresh.\")\n",
    "    return {} # Mengembalikan dict kosong jika tidak ada file atau error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32a0ca",
   "metadata": {},
   "source": [
    "--- Fungsi Utama Pemrosesan ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8f231",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk mengekstrak teks dari semua file PDF di INPUT_DIR,\n",
    "    membersihkannya, dan menyimpan ke file teks mentah dan JSON metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Langkah 1: Pastikan semua direktori yang diperlukan ada.\n",
    "        ensure_directories()\n",
    "        \n",
    "        # Langkah 2: Periksa apakah folder input PDF ada dan berisi file.\n",
    "        if not INPUT_DIR.exists():\n",
    "            logger.error(f\"Input directory not found: {INPUT_DIR}\")\n",
    "            print(f\"Silakan buat folder '{INPUT_DIR}' dan masukkan file PDF di dalamnya.\")\n",
    "            return\n",
    "        \n",
    "        # Langkah 3: Muat data kasus yang sudah ada untuk menghindari pemrosesan ulang.\n",
    "        # Menggunakan OUTPUT_CASES_JSON sebagai sumber utama untuk data yang sudah diproses.\n",
    "        existing_cases_data = load_existing_cases_data(OUTPUT_CASES_JSON)\n",
    "        logger.info(f\"Loaded {len(existing_cases_data)} existing cases from {OUTPUT_CASES_JSON}.\")\n",
    "        \n",
    "        # Langkah 4: Dapatkan daftar semua file PDF di direktori input.\n",
    "        pdf_files = list(INPUT_DIR.glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            logger.warning(f\"No PDF files found in {INPUT_DIR}\")\n",
    "            print(f\"Tidak ada file PDF di folder '{INPUT_DIR}'\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Found {len(pdf_files)} PDF files to process.\")\n",
    "        print(f\"🔍 Ditemukan {len(pdf_files)} file PDF untuk diproses.\")\n",
    "        \n",
    "        # Variabel untuk melacak status pemrosesan\n",
    "        processed_count = 0\n",
    "        failed_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        # Daftar untuk menyimpan semua metadata kasus yang baru diproses\n",
    "        all_processed_cases_metadata = list(existing_cases_data.values())\n",
    "\n",
    "        # Kamus untuk menyimpan teks mentah yang diekstrak untuk referensi\n",
    "        extracted_raw_texts = {} \n",
    "        \n",
    "        # Langkah 5: Iterasi (ulang) melalui setiap file PDF.\n",
    "        for pdf_file_path in tqdm(pdf_files, desc=\"Memproses PDF\"):\n",
    "            try:\n",
    "                filename = pdf_file_path.name\n",
    "                file_stem = pdf_file_path.stem # Nama file tanpa ekstensi\n",
    "                \n",
    "                # Cek apakah file ini sudah diproses sebelumnya\n",
    "                # Membandingkan stem nama file, bukan case_id lengkap yang termasuk timestamp.\n",
    "                # Ini lebih robust untuk pengecekan duplikasi dari file asli.\n",
    "                is_already_processed = False\n",
    "                for existing_case_id in existing_cases_data.keys():\n",
    "                    if file_stem.lower() in existing_case_id.lower():\n",
    "                        is_already_processed = True\n",
    "                        break\n",
    "\n",
    "                if is_already_processed:\n",
    "                    logger.info(f\"File {filename} (stem: {file_stem}) already processed, skipping.\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"Processing: {filename}\")\n",
    "                \n",
    "                # Ekstrak teks dari PDF\n",
    "                full_text = extract_text_from_pdf(pdf_file_path)\n",
    "                \n",
    "                # Periksa apakah ekstraksi berhasil dan teks cukup panjang.\n",
    "                if not full_text or len(full_text.strip()) < 200: # Batas minimal 200 karakter\n",
    "                    logger.error(f\"Failed to extract sufficient text from {filename} or text too short.\")\n",
    "                    failed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Hasilkan case_id unik, termasuk timestamp untuk memastikan keunikan setiap kali diproses\n",
    "                case_id = f\"{generate_case_id_from_filename(file_stem)}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "                # Simpan teks mentah ke file .txt di direktori RAW_TXT_DIR\n",
    "                txt_filename = RAW_TXT_DIR / f\"{case_id}.txt\"\n",
    "                if not save_text_file(full_text, txt_filename):\n",
    "                    failed_count += 1\n",
    "                    continue # Lanjutkan ke file berikutnya jika gagal menyimpan\n",
    "                \n",
    "                # Tambahkan metadata kasus ke daftar\n",
    "                # Ini akan menjadi dasar untuk 02_case_representation.py\n",
    "                case_metadata = {\n",
    "                    \"case_id\": case_id,\n",
    "                    \"file_name\": filename,\n",
    "                    \"file_size\": os.path.getsize(pdf_file_path), # Ukuran file PDF asli\n",
    "                    \"raw_text_file\": str(txt_filename), # Path ke file teks mentah\n",
    "                    \"processed_at\": datetime.now().isoformat(),\n",
    "                    \"extracted_text_preview\": full_text[:500] + \"...\" if len(full_text) > 500 else full_text # Pratinjau teks\n",
    "                }\n",
    "                all_processed_cases_metadata.append(case_metadata)\n",
    "                \n",
    "                # Opsional: simpan teks mentah ke kamus jika Anda ingin menggabungkannya nanti\n",
    "                # Ini bisa menjadi sangat besar dan mungkin tidak perlu disimpan jika hanya untuk debugging\n",
    "                extracted_raw_texts[case_id] = full_text\n",
    "                \n",
    "                processed_count += 1\n",
    "                logger.info(f\"Successfully processed {filename} -> {case_id}\")\n",
    "                \n",
    "                # Simpan progress ke file JSON setiap beberapa file yang berhasil diproses.\n",
    "                if processed_count % 5 == 0:\n",
    "                    # Simpan daftar metadata kasus (penting untuk pipeline selanjutnya)\n",
    "                    save_json_file(all_processed_cases_metadata, OUTPUT_CASES_JSON)\n",
    "                    # Simpan teks mentah yang diekstrak (opsional)\n",
    "                    # save_json_file(extracted_raw_texts, OUTPUT_TEXT_MAP_JSON) \n",
    "                    logger.info(f\"Progress saved: {processed_count} files processed so far.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {filename}: {e}\", exc_info=True) # exc_info=True untuk traceback\n",
    "                failed_count += 1\n",
    "                continue\n",
    "        \n",
    "        # Langkah 6: Simpan semua metadata kasus yang berhasil diproses ke file JSON final.\n",
    "        if all_processed_cases_metadata:\n",
    "            save_json_file(all_processed_cases_metadata, OUTPUT_CASES_JSON)\n",
    "            # Simpan juga teks mentah yang diekstrak jika diperlukan\n",
    "            # save_json_file(extracted_raw_texts, OUTPUT_TEXT_MAP_JSON)\n",
    "        else:\n",
    "            logger.warning(\"No cases were successfully processed. No output JSON created.\")\n",
    "            \n",
    "        # Langkah 7: Tampilkan ringkasan pemrosesan.\n",
    "        print(f\"\\n=== RINGKASAN PEMROSESAN ===\")\n",
    "        print(f\"Total file PDF ditemukan: {len(pdf_files)}\")\n",
    "        print(f\"Berhasil diproses: {processed_count}\")\n",
    "        print(f\"Gagal diproses: {failed_count}\")\n",
    "        print(f\"Dilewati (sudah ada): {skipped_count}\")\n",
    "        print(f\"Total kasus dalam database (termasuk yang sudah ada): {len(all_processed_cases_metadata)}\")\n",
    "        \n",
    "        if processed_count > 0:\n",
    "            print(f\"\\nFile hasil tersimpan di:\")\n",
    "            print(f\"- Raw text files: {RAW_TXT_DIR}\")\n",
    "            print(f\"- Processed cases metadata: {OUTPUT_CASES_JSON}\")\n",
    "            # print(f\"- Extracted raw texts (optional): {OUTPUT_TEXT_MAP_JSON}\") # Jika Anda memilih untuk menyimpannya\n",
    "        print(\"=============================\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Critical error in main function: {e}\", exc_info=True)\n",
    "        print(f\"Terjadi error kritis pada pemrosesan: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91946397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Titik Masrip Skrip ---\n",
    "# Memastikan fungsi main() dijalankan hanya ketika skrip dieksekusi langsung.\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
