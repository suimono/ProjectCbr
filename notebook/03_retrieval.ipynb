{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf69b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da09529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Konfigurasi ---\n",
    "CASE_BASE_PATH = Path(\"data/processed/cases.json\")\n",
    "QUERY_PATH = Path(\"data/eval/queries.json\")\n",
    "OUTPUT_PATH = Path(\"data/results/retrieved_cases.json\")\n",
    "TOP_K_SIMILAR_CASES = 5 # Jumlah kasus serupa teratas yang akan diambil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ebc45",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d31ee",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def initialize_directories() -> bool:\n",
    "    \"\"\"\n",
    "    Memastikan direktori keluaran untuk hasil retrieval ada.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True jika pembuatan direktori berhasil, False jika sebaliknya\n",
    "    \"\"\"\n",
    "    try:\n",
    "        OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Output directory '{OUTPUT_PATH.parent}' ensured.\")\n",
    "        return True\n",
    "    except OSError as e:\n",
    "        logger.error(f\"Error creating directory {OUTPUT_PATH.parent}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e1c9b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_json_data(file_path: Path) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Memuat dan memvalidasi data JSON dari file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (Path): Path ke file JSON.\n",
    "        \n",
    "    Returns:\n",
    "        Optional[List[Dict[str, Any]]]: Daftar data jika berhasil, None jika sebaliknya.\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        logger.error(f\"File '{file_path}' not found.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        if not isinstance(data, list):\n",
    "            logger.error(f\"Invalid data format in '{file_path}'. Expected a JSON array, got {type(data).__name__}.\")\n",
    "            return None\n",
    "            \n",
    "        if not data:\n",
    "            logger.warning(f\"No data found in '{file_path}'.\")\n",
    "            return []\n",
    "            \n",
    "        logger.info(f\"Successfully loaded {len(data)} entries from '{file_path}'.\")\n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to parse JSON from '{file_path}'. Invalid JSON format: {e}\")\n",
    "        return None\n",
    "    except UnicodeDecodeError as e:\n",
    "        logger.error(f\"Encoding error reading '{file_path}': {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error reading '{file_path}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34b360",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_case_text_for_retrieval(case: Dict[str, Any]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Mengekstrak teks yang paling relevan dari sebuah kasus untuk tujuan retrieval.\n",
    "    Prioritas diberikan pada 'ringkasan_fakta', diikuti oleh kombinasi bidang lainnya.\n",
    "    Ini mirip dengan logika create_query_text tetapi disesuaikan untuk representasi kasus.\n",
    "    \n",
    "    Args:\n",
    "        case (Dict[str, Any]): Kamus yang merepresentasikan satu kasus.\n",
    "        \n",
    "    Returns:\n",
    "        Optional[str]: Teks yang diekstrak dari kasus, atau None jika tidak ada teks yang cocok ditemukan.\n",
    "    \"\"\"\n",
    "    # Urutan prioritas bidang untuk mengekstrak teks kasus\n",
    "    field_combinations = [\n",
    "        [\"ringkasan_fakta\"],\n",
    "        [\"jenis_perkara\", \"pasal\", \"status_hukuman\"],\n",
    "        [\"jenis_perkara\", \"pasal\"],\n",
    "        [\"no_perkara\", \"jenis_perkara\", \"tanggal\"],\n",
    "    ]\n",
    "    \n",
    "    for fields_to_try in field_combinations:\n",
    "        text_parts = []\n",
    "        for field in fields_to_try:\n",
    "            if field in case and isinstance(case[field], str):\n",
    "                value = case[field].strip()\n",
    "                # Lewati jika itu placeholder, terlalu pendek, atau hanya karakter berulang\n",
    "                if (value and \n",
    "                    value not in [\"===\", \"---\", \"...\", \"N/A\", \"null\", \"undefined\"] and\n",
    "                    len(set(value)) > 1 and \n",
    "                    len(value) >= 10):\n",
    "                    \n",
    "                    # Pemotongan teks untuk menjaga relevansi\n",
    "                    if field == \"pasal\" and len(value) > 200:\n",
    "                        value = value[:200] + \"...\"\n",
    "                    elif field == \"status_hukuman\" and len(value) > 300:\n",
    "                        value = value[:300] + \"...\"\n",
    "                    \n",
    "                    text_parts.append(value)\n",
    "        \n",
    "        if text_parts:\n",
    "            # Gabungkan bagian teks. Gunakan titik sebagai pemisah.\n",
    "            return \". \".join(text_parts)\n",
    "            \n",
    "    return None # Tidak ada teks yang cocok ditemukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221812b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk melakukan proses retrieval kasus.\n",
    "    \"\"\"\n",
    "    if not initialize_directories():\n",
    "        return\n",
    "\n",
    "    # Muat data kasus\n",
    "    cases = load_json_data(CASE_BASE_PATH)\n",
    "    if cases is None:\n",
    "        logger.error(\"Failed to load case base data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Muat data kueri\n",
    "    queries = load_json_data(QUERY_PATH)\n",
    "    if queries is None:\n",
    "        logger.error(\"Failed to load queries data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    if not cases:\n",
    "        logger.warning(\"No cases found in the case base. Retrieval cannot proceed.\")\n",
    "        return\n",
    "    if not queries:\n",
    "        logger.warning(\"No queries found. Retrieval cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # Ekstrak teks yang relevan dari kasus dan kueri\n",
    "    case_ids = []\n",
    "    case_texts = []\n",
    "    for i, case in enumerate(cases):\n",
    "        if not isinstance(case, dict):\n",
    "            logger.warning(f\"Skipping case at index {i}: not a dictionary.\")\n",
    "            continue\n",
    "        extracted_text = extract_case_text_for_retrieval(case)\n",
    "        if extracted_text:\n",
    "            case_ids.append(case.get(\"case_id\", f\"case_{i}\"))\n",
    "            case_texts.append(extracted_text)\n",
    "        else:\n",
    "            logger.warning(f\"Skipping case {case.get('case_id', f'case_{i}')} due to no suitable text content.\")\n",
    "\n",
    "    query_ids = []\n",
    "    query_texts = []\n",
    "    for i, query in enumerate(queries):\n",
    "        if not isinstance(query, dict):\n",
    "            logger.warning(f\"Skipping query at index {i}: not a dictionary.\")\n",
    "            continue\n",
    "        # Gunakan bidang 'text' yang sudah diproses dari file queries.json\n",
    "        query_text = query.get(\"text\")\n",
    "        if query_text and isinstance(query_text, str) and query_text.strip():\n",
    "            query_ids.append(query.get(\"query_id\", f\"query_{i}\"))\n",
    "            query_texts.append(query_text.strip())\n",
    "        else:\n",
    "            logger.warning(f\"Skipping query {query.get('query_id', f'query_{i}')} due to missing or empty 'text' field.\")\n",
    "\n",
    "    if not case_texts:\n",
    "        logger.error(\"No valid case texts extracted for retrieval. Cannot proceed.\")\n",
    "        return\n",
    "    if not query_texts:\n",
    "        logger.error(\"No valid query texts extracted for retrieval. Cannot proceed.\")\n",
    "        return\n",
    "        \n",
    "    logger.info(f\"Extracted {len(case_texts)} valid case texts and {len(query_texts)} valid query texts.\")\n",
    "\n",
    "    # Gabungkan semua teks untuk TF-IDF fit\n",
    "    corpus = case_texts + query_texts\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    logger.info(\"Performing TF-IDF vectorization...\")\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) # Batasi fitur untuk performa\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    logger.info(f\"TF-IDF matrix created with {tfidf_matrix.shape[1]} features.\")\n",
    "\n",
    "    # Pisahkan matriks kembali menjadi bagian kasus dan kueri\n",
    "    case_matrix = tfidf_matrix[:len(case_texts)]\n",
    "    query_matrix = tfidf_matrix[len(case_texts):]\n",
    "\n",
    "    # Hitung cosine similarity\n",
    "    logger.info(\"Calculating cosine similarities...\")\n",
    "    similarities = cosine_similarity(query_matrix, case_matrix)\n",
    "    logger.info(\"Cosine similarity calculation complete.\")\n",
    "\n",
    "    # Ambil top-K similar case untuk setiap kueri\n",
    "    results = []\n",
    "    for i, sim_scores in enumerate(similarities):\n",
    "        # Menggunakan argpartition untuk kinerja yang lebih baik pada array besar\n",
    "        # Mengambil TOP_K_SIMILAR_CASES indeks terbesar\n",
    "        top_indices = sim_scores.argpartition(-TOP_K_SIMILAR_CASES)[-TOP_K_SIMILAR_CASES:]\n",
    "        \n",
    "        # Urutkan indeks-indeks ini berdasarkan skor similaritasnya secara menurun\n",
    "        sorted_top_indices = top_indices[sim_scores[top_indices].argsort()[::-1]]\n",
    "\n",
    "        top_cases_ids = [case_ids[j] for j in sorted_top_indices]\n",
    "        similarity_scores = [float(sim_scores[j]) for j in sorted_top_indices] # Konversi ke float untuk JSON serialisasi\n",
    "\n",
    "        results.append({\n",
    "            \"query_id\": query_ids[i],\n",
    "            \"top_k_case_ids\": top_cases_ids,\n",
    "            \"similarity_scores\": similarity_scores\n",
    "        })\n",
    "    logger.info(f\"Top {TOP_K_SIMILAR_CASES} similar cases retrieved for {len(results)} queries.\")\n",
    "\n",
    "    # Simpan hasil\n",
    "    try:\n",
    "        with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "        logger.info(f\"âœ… Retrieval completed. Results saved to '{OUTPUT_PATH}'\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write retrieval results to '{OUTPUT_PATH}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a97da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Titik Masuk ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
